{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lB2z9vC-Yvjs"
   },
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oi5Gwo9laWgl",
    "outputId": "7143342f-6878-464c-8ada-8751dec9b1e3"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'NASA/PC5.csv'\n",
    "\n",
    "data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "nameOfHeadersWithLable = data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u0V1yBlzaC1W"
   },
   "outputs": [],
   "source": [
    "# transform data\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "reduced_df = pd.DataFrame(scaled, columns=nameOfHeadersWithLable)\n",
    "reduced_df.to_csv('FirstlyScaledData.csv')\n",
    "# display(reduced_df)\n",
    "# display(scaled)\n",
    "\n",
    "data2 = pd.read_csv('FirstlyScaledData.csv')\n",
    "y = data2.Defective\n",
    "X = data2.drop('Defective', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "\n",
    "nameOfHeaders = X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qT8l3m9daIjy"
   },
   "outputs": [],
   "source": [
    "# AutoEncoder Model Preparation\n",
    "n_inputs = X.shape[1]\n",
    "# define encoder\n",
    "input_data_shape= Input(shape=(n_inputs,))\n",
    "# encoder level 1\n",
    "encoder= Dense(n_inputs*2)(input_data_shape)\n",
    "encoder = BatchNormalization()(encoder)\n",
    "encoder= LeakyReLU()(encoder)\n",
    "# encoder level 2\n",
    "encoder= Dense(n_inputs)(encoder)\n",
    "encoder= BatchNormalization()(encoder)\n",
    "encoder= LeakyReLU()(encoder)\n",
    "# bottleneck\n",
    "n_bottleneck = round(float(n_inputs) / 2.0)\n",
    "bottleneck = Dense(n_bottleneck)(encoder)\n",
    "# define decoder, level 1\n",
    "decoder = Dense(n_inputs)(bottleneck)\n",
    "decoder = BatchNormalization()(decoder)\n",
    "decoder = LeakyReLU()(decoder)\n",
    "# decoder level 2\n",
    "decoder = Dense(n_inputs*2)(decoder)\n",
    "decoder = BatchNormalization()(decoder)\n",
    "decoder = LeakyReLU()(decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pwUDtLSrbcUo"
   },
   "outputs": [],
   "source": [
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(decoder)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=input_data_shape, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2p1gbBabc2Y",
    "outputId": "f6453b36-3a43-45fb-b43c-3ff2de7a6f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 39)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 78)                3120      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 78)               312       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 78)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 39)                3081      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 39)               156       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 39)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                800       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 39)                819       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 39)               156       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 39)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 78)                3120      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 78)               312       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 78)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 39)                3081      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,957\n",
      "Trainable params: 14,489\n",
      "Non-trainable params: 468\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/50\n",
      "849/849 - 4s - loss: 0.0372 - accuracy: 0.0292 - val_loss: 0.1959 - val_accuracy: 0.0781 - 4s/epoch - 5ms/step\n",
      "Epoch 2/50\n",
      "849/849 - 2s - loss: 0.0304 - accuracy: 0.0107 - val_loss: 0.1792 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 3/50\n",
      "849/849 - 2s - loss: 0.0291 - accuracy: 0.0179 - val_loss: 0.0645 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 4/50\n",
      "849/849 - 2s - loss: 0.0267 - accuracy: 0.0234 - val_loss: 0.1768 - val_accuracy: 0.0021 - 2s/epoch - 2ms/step\n",
      "Epoch 5/50\n",
      "849/849 - 2s - loss: 0.0255 - accuracy: 0.0275 - val_loss: 0.0620 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 6/50\n",
      "849/849 - 2s - loss: 0.0251 - accuracy: 0.0283 - val_loss: 0.0933 - val_accuracy: 0.2487 - 2s/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "849/849 - 2s - loss: 0.0243 - accuracy: 0.0248 - val_loss: 0.8030 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 8/50\n",
      "849/849 - 2s - loss: 0.0241 - accuracy: 0.0216 - val_loss: 0.0759 - val_accuracy: 2.9472e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "849/849 - 2s - loss: 0.0240 - accuracy: 0.0198 - val_loss: 0.1796 - val_accuracy: 2.9472e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 10/50\n",
      "849/849 - 2s - loss: 0.0235 - accuracy: 0.0195 - val_loss: 0.1896 - val_accuracy: 8.8417e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "849/849 - 2s - loss: 0.0233 - accuracy: 0.0181 - val_loss: 0.0676 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 12/50\n",
      "849/849 - 2s - loss: 0.0228 - accuracy: 0.0312 - val_loss: 1.5905 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 13/50\n",
      "849/849 - 2s - loss: 0.0227 - accuracy: 0.0315 - val_loss: 0.4697 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 14/50\n",
      "849/849 - 2s - loss: 0.0226 - accuracy: 0.0312 - val_loss: 0.0328 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 15/50\n",
      "849/849 - 2s - loss: 0.0221 - accuracy: 0.0282 - val_loss: 0.3033 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 16/50\n",
      "849/849 - 2s - loss: 0.0221 - accuracy: 0.0257 - val_loss: 0.3935 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 17/50\n",
      "849/849 - 2s - loss: 0.0221 - accuracy: 0.0172 - val_loss: 6.9338 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "849/849 - 2s - loss: 0.0221 - accuracy: 0.0260 - val_loss: 0.7375 - val_accuracy: 0.0999 - 2s/epoch - 3ms/step\n",
      "Epoch 19/50\n",
      "849/849 - 2s - loss: 0.0225 - accuracy: 0.0186 - val_loss: 4.9545 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 20/50\n",
      "849/849 - 2s - loss: 0.0225 - accuracy: 0.0271 - val_loss: 0.0284 - val_accuracy: 0.0215 - 2s/epoch - 3ms/step\n",
      "Epoch 21/50\n",
      "849/849 - 2s - loss: 0.0220 - accuracy: 0.0335 - val_loss: 2.0371 - val_accuracy: 0.0251 - 2s/epoch - 3ms/step\n",
      "Epoch 22/50\n",
      "849/849 - 2s - loss: 0.0219 - accuracy: 0.0316 - val_loss: 0.2107 - val_accuracy: 0.0000e+00 - 2s/epoch - 3ms/step\n",
      "Epoch 23/50\n",
      "849/849 - 2s - loss: 0.0222 - accuracy: 0.0226 - val_loss: 1.2706 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "849/849 - 2s - loss: 0.0219 - accuracy: 0.0304 - val_loss: 0.1404 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "849/849 - 2s - loss: 0.0215 - accuracy: 0.0283 - val_loss: 0.1062 - val_accuracy: 8.8417e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "849/849 - 2s - loss: 0.0217 - accuracy: 0.0301 - val_loss: 0.0276 - val_accuracy: 0.0675 - 2s/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "849/849 - 2s - loss: 0.0208 - accuracy: 0.0290 - val_loss: 0.0410 - val_accuracy: 0.0115 - 2s/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "849/849 - 2s - loss: 0.0213 - accuracy: 0.0343 - val_loss: 0.5719 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "849/849 - 2s - loss: 0.0213 - accuracy: 0.0234 - val_loss: 0.6181 - val_accuracy: 0.1833 - 2s/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "849/849 - 2s - loss: 0.0224 - accuracy: 0.0192 - val_loss: 1.6167 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "849/849 - 2s - loss: 0.0213 - accuracy: 0.0123 - val_loss: 0.0416 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "849/849 - 2s - loss: 0.0213 - accuracy: 0.0127 - val_loss: 0.1301 - val_accuracy: 2.9472e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "849/849 - 2s - loss: 0.0225 - accuracy: 0.0133 - val_loss: 0.1163 - val_accuracy: 2.9472e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "849/849 - 2s - loss: 0.0224 - accuracy: 0.0153 - val_loss: 0.0353 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "849/849 - 2s - loss: 0.0217 - accuracy: 0.0188 - val_loss: 0.3453 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 36/50\n"
     ]
    }
   ],
   "source": [
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# plot the autoencoder\n",
    "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=2, validation_data=(X_test,y_test))\n",
    "# # plot loss\n",
    "# pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRS71Ex1cUE4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(50), acc, label='Training Accuracy')\n",
    "plt.plot(range(50), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Figure (a) Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(50), loss, label='Training Loss')\n",
    "plt.plot(range(50), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Figure (b) Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=input_data_shape, outputs=bottleneck)\n",
    "# save the encoder to file\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H8gAIqjcgzm",
    "outputId": "d06b565f-8813-4cd4-ad2b-8672f57db815"
   },
   "outputs": [],
   "source": [
    "#Building a Base Model to compare the performance after compressing the data using Encoder model.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define model\n",
    "model = RandomForestClassifier()\n",
    "# fit model on training set\n",
    "model.fit(X_train, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test)\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmIqzAtndGc6",
    "outputId": "dbf51d6d-e7e0-4122-e330-b5525c4fb341"
   },
   "outputs": [],
   "source": [
    "#Compressing the input data using Encoder Model and fitting it on the Logistic Regression model.\n",
    "# load the model from file\n",
    "encoder = load_model('encoder.h5')\n",
    "\n",
    "encoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "# nameOfHeaders = X_train.columns.values\n",
    "# reduced_df = pd.DataFrame(encoder.predict(x_test), columns=nameOfHeaders)\n",
    "\n",
    "# reduced_df.to_csv('PredictedJM1OnTestData.csv')\n",
    "\n",
    "\n",
    "# define the model\n",
    "random_forest = RandomForestClassifier(max_depth=50)\n",
    "# fit the model on the training set\n",
    "random_forest.fit(X_train_encode, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = random_forest.predict(X_test_encode)\n",
    "# calculate classification accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal number of features: {}'.format(random_forest.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(random_forest.feature_importances_)\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Random Forest with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(random_forest.feature_importances_) + 1), random_forest.feature_importances_, color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AutoEncoders as Dimensionality Reduction Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
